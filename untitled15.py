# -*- coding: utf-8 -*-
"""Untitled15.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lVInPEUsohFyoKCOafMxhZl7MVxSvrFN
"""

import numpy as np
import matplotlib.pyplot as plt

# Define the GridWorld environment dynamics
def find_next_state(state, action):
    """
    Returns the next state and reward given the current state and action.
    - Red states (10, 11, 13, 14): Return to start (20) with -20 reward.
    - Terminal states (0, 4): End episode with 0 reward.
    - Wall collisions or normal moves: -1 reward.
    """
    if state in [10, 11, 13, 14]:
        raise ValueError("Agent shouldn't start from a red state")
    if state in [0, 4]:
        return state, 0
    # Wall collisions — disallow movement and give penalty
    if state in [0, 5, 15, 20] and action == 2:  # left
        return state, -1
    if state in [1, 2, 3] and action == 0:      # up
        return state, -1
    if state in [20, 21, 22, 23, 24] and action == 1:  # down
        return state, -1
    if state in [4, 9, 19, 24] and action == 3:  # right
        return state, -1
    # Compute next state based on action
    if action == 0: next_state = state - 5      # up
    elif action == 1: next_state = state + 5    # down
    elif action == 2: next_state = state - 1    # left
    elif action == 3: next_state = state + 1    # right
    # If next state is red, send agent back to start with heavy penalty
    if next_state in [10, 11, 13, 14]:
        return 20, -20
    # If next state is terminal, end episode with 0 reward
    if next_state in [0, 4]:
        return next_state, 0
    # Normal move to a safe state
    return next_state, -1

# Epsilon-greedy strategy for balancing exploration and exploitation
def select_action(Q, state, epsilon=0.1):
    """
    Selects an action using ε-greedy policy.
    - With probability ε (0.1), choose a random action (exploration).
    - Otherwise, choose the action with the highest Q-value (exploitation).
    """
    if np.random.rand() < epsilon:
        return np.random.choice([0, 1, 2, 3])  # Explore
    return np.argmax(Q[state])            # Exploit best known action

# Plot total reward per episode
def plot_rewards(rewards, title="Reward Pattern"):
    """
    Plots the accumulated rewards for a single method.
    """
    plt.figure()
    plt.plot(rewards)
    plt.title(title)
    plt.xlabel("Episode")
    plt.ylabel("Accumulated Reward")
    plt.grid(True)
    plt.show()

# Plot combined rewards for comparison
def plot_combined_rewards(sarsa_rewards, qlearning_rewards):
    """
    Plots Sarsa and Q-learning rewards in one figure for comparison.
    """
    plt.figure()
    plt.plot(sarsa_rewards, label="Sarsa")
    plt.plot(qlearning_rewards, label="Q-learning")
    plt.title("Sarsa vs Q-learning Reward Pattern")
    plt.xlabel("Episode")
    plt.ylabel("Accumulated Reward")
    plt.grid(True)
    plt.legend()
    plt.show()

# Plot the trajectory of the agent
def plot_trajectory(policy, start_state=20, title="Trajectory"):
    """
    Simulates and plots the trajectory from start_state using the given policy.
    Returns the list of states visited.
    """
    states = [start_state]
    state = start_state
    while state not in [0, 4]:
        action = policy[state]
        next_state, _ = find_next_state(state, action)
        states.append(next_state)
        state = next_state
    action_names = {0: "up", 1: "down", 2: "left", 3: "right"}
    print(f"{title}: {' -> '.join([f'{s} ({action_names[policy[s]]})' for s in states[:-1]])} -> {states[-1]}")
    grid = np.zeros((5, 5))
    for s in states:
        row, col = divmod(s, 5)
        grid[row, col] = 1  # Path
    for red in [10, 11, 13, 14]:
        row, col = divmod(red, 5)
        grid[row, col] = -1  # Red states
    grid[0, 0] = 2  # Start (blue)
    grid[0, 4] = 2  # Terminal (black)
    plt.figure()
    plt.title(title)
    plt.imshow(grid, cmap='RdBu', vmin=-1, vmax=2)
    plt.grid(True)
    plt.xticks(range(5))
    plt.yticks(range(5))
    plt.show()
    return states

# Compare trajectories
def compare_trajectories(sarsa_traj, qlearning_traj):
    """
    Compares the trajectories of Sarsa and Q-learning and explains similarities/differences.
    """
    print("\nTrajectory Comparison:")
    if sarsa_traj == qlearning_traj:
        print("Sarsa and Q-learning trajectories are identical.")
        print("Reason: Both algorithms converge to the optimal policy in a deterministic environment after sufficient episodes.")
    else:
        print("Sarsa and Q-learning trajectories differ.")
        print("Reason: Sarsa (on-policy) may prefer safer paths due to its consideration of the exploration policy, while Q-learning (off-policy) directly targets the optimal policy, potentially leading to different paths if convergence is incomplete.")

# Sarsa Agent class
class SarsaAgent:
    def __init__(self, alpha=0.5, gamma=0.95):
        """
        Initializes the Sarsa agent with a Q-table (25 states x 4 actions).
        - alpha: Learning rate
        - gamma: Discount factor
        """
        self.Q = np.random.rand(25, 4)  # Initialize Q-table
        self.Q[0, :] = 0  # Terminal states
        self.Q[4, :] = 0
        self.alpha = alpha
        self.gamma = gamma
        self.rewards = []

    def train(self, episodes=500, epsilon=0.1):
        """
        Trains the Sarsa agent for the specified number of episodes using ε-greedy action selection.
        """
        for ep in range(episodes):
            state = 20  # Start from the blue square
            action = select_action(self.Q, state, epsilon)
            total_reward = 0
            while True:
                next_state, reward = find_next_state(state, action)
                total_reward += reward
                next_action = select_action(self.Q, next_state, epsilon)
                # SARSA update rule
                self.Q[state, action] += self.alpha * (
                    reward + self.gamma * self.Q[next_state, next_action] - self.Q[state, action]
                )
                if next_state in [0, 4]:
                    break
                state = next_state
                action = next_action
            self.rewards.append(total_reward)

    def get_policy(self):
        """
        Returns the optimal policy as a 25-element array.
        """
        return np.argmax(self.Q, axis=1)

# Q-Learning Agent class
class QLearningAgent:
    def __init__(self, alpha=0.5, gamma=0.95):
        """
        Initializes the Q-learning agent with a Q-table (25 states x 4 actions).
        """
        self.Q = np.random.rand(25, 4)
        self.Q[0, :] = 0
        self.Q[4, :] = 0
        self.alpha = alpha
        self.gamma = gamma
        self.rewards = []

    def train(self, episodes=500, epsilon=0.1):
        """
        Trains the Q-learning agent for the specified number of episodes using ε-greedy action selection.
        """
        for ep in range(episodes):
            state = 20
            total_reward = 0
            while True:
                action = select_action(self.Q, state, epsilon)
                next_state, reward = find_next_state(state, action)
                total_reward += reward
                # Q-learning update: use max future value
                best_next = np.max(self.Q[next_state])
                td_target = reward + self.gamma * best_next
                td_error = td_target - self.Q[state, action]
                self.Q[state, action] += self.alpha * td_error
                if next_state in [0, 4]:
                    break
                state = next_state
            self.rewards.append(total_reward)

    def get_policy(self):
        """
        Returns the optimal policy as a 25-element array.
        """
        return np.argmax(self.Q, axis=1)

# Main execution
if __name__ == "__main__":
    np.random.seed(42)  # For reproducibility
    # Train Sarsa agent
    sarsa = SarsaAgent(alpha=0.5, gamma=0.95)
    sarsa.train(episodes=300, epsilon=0.1)
    sarsa_policy = sarsa.get_policy()
    print("Sarsa Optimal Policy (0=up, 1=down, 2=left, 3=right):")
    print(sarsa_policy.reshape(5, 5))
    plot_rewards(sarsa.rewards, "SARSA Reward Pattern")
    sarsa_traj = plot_trajectory(sarsa_policy, title="Sarsa Trajectory")

    # Train Q-learning agent
    qlearning = QLearningAgent(alpha=0.5, gamma=0.95)
    qlearning.train(episodes=300, epsilon=0.1)
    qlearning_policy = qlearning.get_policy()
    print("Q-learning Optimal Policy (0=up, 1=down, 2=left, 3=right):")
    print(qlearning_policy.reshape(5, 5))
    plot_rewards(qlearning.rewards, "Q-learning Reward Pattern")
    qlearning_traj = plot_trajectory(qlearning_policy, title="Q-learning Trajectory")

    # Plot combined rewards
    plot_combined_rewards(sarsa.rewards, qlearning.rewards)

    # Compare trajectories
    compare_trajectories(sarsa_traj, qlearning_traj)

    # Enhanced analysis of reward behavior
    print("\nReward Behavior Analysis:")
    print("Sarsa: The sum of rewards per episode starts highly negative (e.g., -20 or worse) because the ϵ-greedy policy (ε=0.1) causes the agent to occasionally explore red states, incurring a -20 penalty. As training progresses, the agent learns to avoid red states (10, 11, 13, 14) and navigate through the opening (state 12) to a terminal state (0 or 4). Rewards converge to approximately -6 to -8, reflecting the optimal path length (6-8 steps at -1 per step). The on-policy nature of Sarsa means updates account for exploration, leading to slightly slower convergence.")
    print(f"Average reward in last 50 episodes (Sarsa): {np.mean(sarsa.rewards[-50:]):.2f}")
    print("Q-learning: Rewards follow a similar trend, starting highly negative due to red state penalties. However, Q-learning converges faster because its off-policy updates use the maximum Q-value of the next state, directly targeting the optimal policy. Rewards stabilize at -6 to -8, matching Sarsa, as the environment is deterministic.")
    print(f"Average reward in last 50 episodes (Q-learning): {np.mean(qlearning.rewards[-50:]):.2f}")
    print("Comparison: Both methods show similar convergence due to the deterministic environment. Q-learning's off-policy updates may reduce variance in rewards earlier, but with 300 episodes, both achieve comparable performance. The ϵ-greedy policy ensures sufficient exploration to discover the optimal path while exploiting learned Q-values.")