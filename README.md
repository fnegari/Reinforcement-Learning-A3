```markdown
# Reinforcement Learning Assignment 3

This repository contains the implementation of **Sarsa** and **Q-learning** algorithms for a 5Ã—5 GridWorld problem, submitted for the Reinforcement Learning course (Course Number: 6650) at Memorial University.  
**Assignment 3 â€” Due August 5, 2025**

---

## ğŸ“– Overview

- **Goal:** Learn optimal policies for a 5Ã—5 GridWorld using on-policy (Sarsa) and off-policy (Q-learning) methods.  
- **Action selection:** Îµ-greedy (Îµ = 0.1)  
- **Training:** 300 episodes, Î± = 0.5, Î³ = 0.95  
- **Outputs:**  
  - Optimal policy grids  
  - Trajectories under greedy policy  
  - Reward-per-episode plots  
  - PNG files for inclusion in the report

---

## ğŸŒ GridWorld Environment

```

0   1   2   3   4
5   6   7   8   9
10  11  12  13  14
15  16  17  18  19
20  21  22  23  24

````

- **Start:** State 20 (bottom-left)  
- **Terminal:** States 0, 4 (top row) reward = 0  
- **Red states:** 10, 11, 13, 14 (reward = â€“20, reset to 20)  
- **Rewards:**  
  - Normal move (white cell): â€“1  
  - Wall collision: â€“1  
  - Red cell: â€“20 + reset  
  - Terminal: 0  

---

## âš™ï¸ Implementation

- **Sarsa (On-Policy):**

  \[
  Q(s,a) \leftarrow Q(s,a) + \alpha \bigl[r + \gamma\,Q(s',a') - Q(s,a)\bigr]
  \]

- **Q-learning (Off-Policy):**

  \[
  Q(s,a) \leftarrow Q(s,a) + \alpha \bigl[r + \gamma\,\max_{a'} Q(s',a') - Q(s,a)\bigr]
  \]

- **Îµ-greedy:**  
  - 10% random action  
  - 90% best action

---

## ğŸ›  Prerequisites

- **Python 3.6+**  
- **NumPy**  
- **Matplotlib**

```bash
pip install numpy matplotlib
````

---

## ğŸ“¥ Installation

```bash
git clone https://github.com/[your-username]/rl-assignment3.git
cd rl-assignment3
```

Verify that `untitled15.py` is present and dependencies are installed.

---

## â–¶ï¸ How to Run

```bash
python untitled15.py
```

The script will:

1. Seed RNG for reproducibility (`np.random.seed(42)`).
2. Train Sarsa and Q-learning for 300 episodes.
3. Save plots as:

   * `sarsa_trajectory.png`
   * `qlearning_trajectory.png`
   * `sarsa_rewards.png`
   * `qlearning_rewards.png`
   * `combined_rewards.png`
4. Print policies, trajectories, and reward analysis to console.

---

## ğŸ¯ Expected Outputs

* **Trajectories (Greedy after training):**

  * **Sarsa:**

    ```
    20 â†’ 21 â†’ 22 â†’ 17 â†’ 12 â†’ 7 â†’ 2 â†’ 1 â†’ 0  (8 steps)
    ```
  * **Q-learning:**

    ```
    20 â†’ 21 â†’ 16 â†’ 17 â†’ 12 â†’ 7 â†’ 6 â†’ 5 â†’ 0  (8 steps)
    ```

* **Reward Analysis (last 50 episodes):**

  * Sarsa: **â€“19.70**
  * Q-learning: **â€“13.34**

---

## ğŸ“Š Results Summary

| Algorithm      | Trajectory                       | Avg. Reward (Last 50 Eps) | Notes                          |
| -------------- | -------------------------------- | ------------------------: | ------------------------------ |
| **Sarsa**      | 20â†’21â†’22â†’17â†’12â†’7â†’2â†’1â†’0 (8 steps) |                    â€“19.70 | On-policy, safer path          |
| **Q-learning** | 20â†’21â†’16â†’17â†’12â†’7â†’6â†’5â†’0 (8 steps) |                    â€“13.34 | Off-policy, faster convergence |

---

## ğŸ”„ Reproducibility

* **Random Seed:** `np.random.seed(42)`
* **Parameters:**

  * Learning rate (Î±): 0.5
  * Discount factor (Î³): 0.95
  * Exploration rate (Îµ): 0.1
  * Episodes: 300
* **Note:** Deterministic environment; only Îµ-greedy introduces randomness.
* **Tip:** Increase episodes (e.g., 1000) or reduce Îµ (e.g., 0.05) for better convergence (optimal \~â€“6 to â€“8).

---

## ğŸ™ Credits

Completed by **Fatemeh Negari** and **Saba Tamizi** for RL Course (6650).

---

## ğŸ“« Contact

For issues or questions, open an issue or contact:
`your.email@domain.com` â€¢ [GitHub](https://github.com/your-username)

```
```
